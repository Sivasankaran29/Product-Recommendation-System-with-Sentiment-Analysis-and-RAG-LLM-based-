{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87f9efd6-c51c-4ce4-80ed-55003e784b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Evaluation Metrics\n",
      "âœ… RMSE (Simulated Ratings): 1.449\n",
      "âœ… Precision@3: 0.67\n",
      "âœ… Sentiment Accuracy: 1.00\n",
      "âœ… Sentiment F1 Score: 1.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score\n",
    "\n",
    "# === Load Processed Data ===\n",
    "df = pd.read_csv(\"D:/flipkart_recommendation/data/final_processed_reviews.csv\")\n",
    "\n",
    "# === Step 1: Simulate Rating from Sentiment Polarity ===\n",
    "df[\"simulated_rating\"] = df[\"sentiment_score\"] * 5\n",
    "\n",
    "# === Step 2: RMSE for Recommendations ===\n",
    "def calculate_rmse(df):\n",
    "    actual = df.groupby(\"product_id\")[\"simulated_rating\"].mean()\n",
    "    predicted = np.random.uniform(2.5, 4.5, len(actual))  # simulated predictions\n",
    "    return np.sqrt(mean_squared_error(actual, predicted))\n",
    "\n",
    "# === Step 3: Precision@K ===\n",
    "def precision_at_k(true_list, predicted_list, k=5):\n",
    "    true_set = set(true_list)\n",
    "    pred_set = set(predicted_list[:k])\n",
    "    return len(true_set & pred_set) / k\n",
    "\n",
    "# === Step 4: F1 & Accuracy for Sentiment (using manually labeled test set) ===\n",
    "def evaluate_sentiment_classification():\n",
    "    test_df = pd.read_csv(\"D:/flipkart_recommendation/data/sentiment_test_labeled.csv\")\n",
    "    \n",
    "    test_df[\"predicted_sentiment\"] = test_df[\"review_text\"].apply(lambda x: (\n",
    "        \"positive\" if TextBlob(x).sentiment.polarity > 0.1\n",
    "        else \"negative\" if TextBlob(x).sentiment.polarity < -0.1\n",
    "        else \"neutral\"\n",
    "    ))\n",
    "    \n",
    "    acc = accuracy_score(test_df[\"true_sentiment\"], test_df[\"predicted_sentiment\"])\n",
    "    f1 = f1_score(test_df[\"true_sentiment\"], test_df[\"predicted_sentiment\"], average='macro')\n",
    "    \n",
    "    return acc, f1\n",
    "\n",
    "# === Run Evaluation ===\n",
    "rmse = calculate_rmse(df)\n",
    "acc, f1 = evaluate_sentiment_classification()\n",
    "p_at_3 = precision_at_k([\"pixel 7a\", \"moto edge\", \"samsung a54\"], [\"pixel 7a\", \"vivo y56\", \"moto edge\"], 3)\n",
    "\n",
    "# === Print Results ===\n",
    "print(\"ðŸ“Š Evaluation Metrics\")\n",
    "print(f\"âœ… RMSE (Simulated Ratings): {rmse:.3f}\")\n",
    "print(f\"âœ… Precision@3: {p_at_3:.2f}\")\n",
    "print(f\"âœ… Sentiment Accuracy: {acc:.2f}\")\n",
    "print(f\"âœ… Sentiment F1 Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4df459-1a2a-4078-b42c-9152600a3148",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
