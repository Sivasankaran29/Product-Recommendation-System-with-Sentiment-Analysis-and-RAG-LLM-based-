{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c04d3915-d7a0-4968-b0e6-b52cf9a5c553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data saved to data/analyzed_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv(\"D:/data/cleaned_final_project_data.csv\")\n",
    "\n",
    "# Drop missing\n",
    "df.dropna(subset=[\"review_text\", \"rating\", \"product_id\"], inplace=True)\n",
    "\n",
    "# Add sentiment\n",
    "df[\"sentiment_label\"] = df[\"review_text\"].apply(\n",
    "    lambda x: TextBlob(x).sentiment.polarity\n",
    ")\n",
    "df[\"sentiment_type\"] = df[\"sentiment_label\"].apply(\n",
    "    lambda x: \"POSITIVE\" if x > 0.1 else \"NEGATIVE\" if x < -0.1 else \"NEUTRAL\"\n",
    ")\n",
    "\n",
    "# Encode user/product if needed\n",
    "df[\"user_id\"] = [\"user_\" + str(i) for i in range(len(df))]\n",
    "\n",
    "le_user = LabelEncoder()\n",
    "le_product = LabelEncoder()\n",
    "\n",
    "df[\"user_id_enc\"] = le_user.fit_transform(df[\"user_id\"])\n",
    "df[\"product_id_enc\"] = le_product.fit_transform(df[\"product_id\"])\n",
    "\n",
    "df.to_csv(\"D:/data/analyzed_reviews.csv\", index=False)\n",
    "print(\"✅ Data saved to data/analyzed_reviews.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f57cceff-ff4a-4dc2-bf55-74f318e34846",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Siva\\AppData\\Local\\Temp\\ipykernel_53028\\2421756775.py:79: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "C:\\Users\\Siva\\anaconda3\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Siva\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Siva\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vector store saved successfully at: D:/flipkart_recommendation/models/vector_store\n",
      "✅ Preprocessed review data saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"D:/flipkart_recommendation/.env\")\n",
    "\n",
    "# === PATHS ===\n",
    "DATA_PATH = \"D:/flipkart_recommendation/data/analyzed_reviews.csv\"\n",
    "VECTOR_STORE_PATH = \"D:/flipkart_recommendation/models/vector_store\"\n",
    "\n",
    "# === STEP 1: Load & Clean Data ===\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.dropna(subset=[\"product_id\", \"user_id\", \"review_text\", \"rating\"], inplace=True)\n",
    "\n",
    "# === STEP 2: Normalize IDs ===\n",
    "df[\"product_id\"] = df[\"product_id\"].astype(str)\n",
    "df[\"user_id\"] = df[\"user_id\"].astype(str)\n",
    "\n",
    "# === STEP 3: Sentiment Analysis ===\n",
    "def get_sentiment_polarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "df[\"sentiment_score\"] = df[\"review_text\"].apply(get_sentiment_polarity)\n",
    "df[\"sentiment_type\"] = df[\"sentiment_score\"].apply(\n",
    "    lambda x: \"positive\" if x > 0.1 else \"negative\" if x < -0.1 else \"neutral\")\n",
    "\n",
    "# === STEP 4: Collaborative Filtering with SVD ===\n",
    "ratings_matrix = df.pivot_table(index=\"user_id\", columns=\"product_id\", values=\"rating\", fill_value=0)\n",
    "\n",
    "n_components = min(20, min(ratings_matrix.shape) - 1)  # dynamically adjust based on matrix size\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "latent_matrix = svd.fit_transform(ratings_matrix)\n",
    "similarity_matrix = cosine_similarity(latent_matrix)\n",
    "\n",
    "user_similarity_df = pd.DataFrame(similarity_matrix, index=ratings_matrix.index, columns=ratings_matrix.index)\n",
    "\n",
    "def recommend_products(user_id, top_n=5):\n",
    "    if user_id not in user_similarity_df.index:\n",
    "        return []\n",
    "    sim_users = user_similarity_df[user_id].sort_values(ascending=False).iloc[1:top_n+1].index\n",
    "    sim_ratings = ratings_matrix.loc[sim_users].mean().sort_values(ascending=False)\n",
    "    already_rated = ratings_matrix.loc[user_id][ratings_matrix.loc[user_id] > 0].index\n",
    "    return sim_ratings.drop(already_rated, errors='ignore').head(top_n).index.tolist()\n",
    "\n",
    "# === STEP 5: Hybrid Recommendation (Ratings + Sentiment) ===\n",
    "def recommend_hybrid(user_id, top_n=5):\n",
    "    if user_id not in user_similarity_df.index:\n",
    "        return []\n",
    "    sim_users = user_similarity_df[user_id].sort_values(ascending=False).iloc[1:top_n+1].index\n",
    "    sim_ratings = ratings_matrix.loc[sim_users].mean().reset_index()\n",
    "    sim_ratings.columns = [\"product_id\", \"avg_rating\"]\n",
    "\n",
    "    avg_sentiment = df.groupby(\"product_id\")[\"sentiment_score\"].mean().reset_index()\n",
    "    avg_sentiment.columns = [\"product_id\", \"avg_sentiment\"]\n",
    "\n",
    "    hybrid_df = pd.merge(sim_ratings, avg_sentiment, on=\"product_id\")\n",
    "    hybrid_df[\"hybrid_score\"] = 0.7 * hybrid_df[\"avg_rating\"] + 0.3 * hybrid_df[\"avg_sentiment\"]\n",
    "\n",
    "    already_rated = ratings_matrix.loc[user_id][ratings_matrix.loc[user_id] > 0].index\n",
    "    hybrid_df = hybrid_df[~hybrid_df[\"product_id\"].isin(already_rated)]\n",
    "\n",
    "    return hybrid_df.sort_values(by=\"hybrid_score\", ascending=False).head(top_n)[\"product_id\"].tolist()\n",
    "\n",
    "# === STEP 6: RAG Vector Store Embedding ===\n",
    "grouped = df.groupby(\"product_id\")[\"review_text\"].apply(lambda x: \" \".join(x)).reset_index()\n",
    "docs = [Document(page_content=row[\"review_text\"], metadata={\"product_id\": row[\"product_id\"]}) for _, row in grouped.iterrows()]\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "split_docs = splitter.split_documents(docs)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(split_docs, embeddings)\n",
    "vectorstore.save_local(VECTOR_STORE_PATH)\n",
    "print(\"✅ Vector store saved successfully at:\", VECTOR_STORE_PATH)\n",
    "\n",
    "# === Optional: Save intermediate data ===\n",
    "df.to_csv(\"D:/flipkart_recommendation/data/final_processed_reviews.csv\", index=False)\n",
    "print(\"✅ Preprocessed review data saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2aa29a-79e2-4e70-a452-861ec4571f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
